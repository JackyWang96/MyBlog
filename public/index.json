[{"content":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Spark core module Spark Core: The most basic and core functions of Spark are provided in Spark Core\nSpark SQL: Spark SQL is the component that Spark uses to manipulate structured data. With Spark SQL, users can query data using SQL or the Apache Hive version of the SQL dialect (HQL).\nSpark Streaming: Spark Streaming is a component on the Spark platform that performs stream computing for real-time data, and provides a rich API for processing data streams.\n2. Spark running model Local model:\nAn environment that executes Spark code locally without requiring any other node resources Standalone model:\nSubmit the application to the corresponding cluster for execution, using the cluster mode running on Spark\u0026rsquo;s own nodes, which is what we call the standalone deployment (Standalone) mode.\nOnline monitor: Use the Master resource to monitor the Web UI interface: http://linux1:8080\nYarn model:\nThe Yarn mode can utilize the resources of the Hadoop cluster, and the Spark application is submitted to the YARN cluster. Provides functions such as dynamic resource allocation and job recovery, suitable for large-scale production environments.\nThe Standalone mode is more suitable for small-scale testing and development environments.\nK8S \u0026amp; Mesos model:\nMesos is an open source distributed resource management framework under Apache. It is known as the kernel of a distributed system. It is widely used in Twitter and manages the application deployment on more than 300,000 Twitter servers.\n","permalink":"https://jackywang96.github.io/BlogSite/post/big-data/spark-learning-2/","summary":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Spark core module Spark Core: The most basic and core functions of Spark are provided in Spark Core\nSpark SQL: Spark SQL is the component that Spark uses to manipulate structured data. With Spark SQL, users can query data using SQL or the Apache Hive version of the SQL dialect (HQL).\nSpark Streaming: Spark Streaming is a component on the Spark platform that performs stream computing for real-time data, and provides a rich API for processing data streams.","title":"Big Data Learning Day-2"},{"content":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Data processing tool Spark and Handoop Apache Spark: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It offers support for various data processing tasks, including SQL queries, machine learning, and graph processing, through built-in libraries like Spark SQL, MLlib, and GraphX.\nApache Hadoop: Apache Hadoop is an open-source, distributed computing framework for processing and storing large amounts of data on clusters of commodity hardware. The core components of Hadoop are the Hadoop Distributed File System (HDFS) and the MapReduce programming model. HDFS is a distributed file system that provides high-throughput access to application data, while MapReduce is a parallel data processing model that simplifies large-scale data processing across distributed clusters.\n2. Data storage tool As big data involves large volumes of data, it requires specialized data storage systems such as HDFS, Amazon S3, or Google Cloud Storage.\n3. Data visualization and analysis Once the data has been processed, it needs to be analyzed and presented in a meaningful way. There are several tools available for data visualization and analysis, such as Tableau, Power BI, and Apache Superset.\n4. Data governance and security As big data involves sensitive data, it\u0026rsquo;s important to have strong data governance and security measures in place. This includes access control, data masking, and encryption.\n","permalink":"https://jackywang96.github.io/BlogSite/post/big-data/spark-learning-1/","summary":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Data processing tool Spark and Handoop Apache Spark: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It offers support for various data processing tasks, including SQL queries, machine learning, and graph processing, through built-in libraries like Spark SQL, MLlib, and GraphX.\nApache Hadoop: Apache Hadoop is an open-source, distributed computing framework for processing and storing large amounts of data on clusters of commodity hardware.","title":"Big Data Learning Day-1"},{"content":"Introduction Tailwind CSS is a CSS styling library that provides us with all the building blocks we need to build custom designs without using custom styles.\nFeature Tailwind will automatically delete all unused CSS when building production files, so its size in the actual production environment is very small, and it is considered to be the CSS style library with the most design sense.\nInstallation //Install Tailwind CSS npm install -D tailwindcss npx tailwindcss init //Add the paths to all of your template files in your tailwind.config.js file. module.exports = { content: [\u0026#34;./src/**/*.{html,js}\u0026#34;], theme: { extend: {}, }, plugins: [], } //Add the Tailwind directives to your CSS @tailwind base; @tailwind components; @tailwind utilities; //Start the Tailwind CLI build process npx tailwindcss -i ./src/input.css -o ./dist/output.css --watch ","permalink":"https://jackywang96.github.io/BlogSite/post/tailwindcss/","summary":"Introduction Tailwind CSS is a CSS styling library that provides us with all the building blocks we need to build custom designs without using custom styles.\nFeature Tailwind will automatically delete all unused CSS when building production files, so its size in the actual production environment is very small, and it is considered to be the CSS style library with the most design sense.\nInstallation //Install Tailwind CSS npm install -D tailwindcss npx tailwindcss init //Add the paths to all of your template files in your tailwind.","title":"TailwindCSS"},{"content":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer. In this case, a third pointer is required at length of nums1. The role of the third pointer is to select where the larger number is placed\nSolution public class MergeSortedArray { public static void merge(int[]nums1, int m, int[]nums2, int n) { //有序数组条件下，设置3个指针，分别放在nums1，nums2和集合数组的末尾 int i=m-1; int j=n-1; int point= nums1.length-1; //判断nums2是否遍历完成 while(j\u0026gt;=0){ //如果nums2中元素小于nums1中的，则把nums1里的元素放到point指向的位置 //之后移动i和point指针 if(i\u0026gt;=0 \u0026amp;\u0026amp; nums1[i]\u0026gt;nums2[j]){ nums1[point]=nums1[i]; i--; point--; } //nums2的元素较大情况直接把nums2中最大元素放置到数组最右端。 //移动j和point指针 else{ nums1[point]=nums2[j]; point--; j--; } } } ","permalink":"https://jackywang96.github.io/BlogSite/post/fast-slow-pointer-floyd-circle-method/fast-slow-pointer/","summary":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer.","title":"Fast,Slow pointer--Floyd circle method"},{"content":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer. In this case, a third pointer is required at length of nums1. The role of the third pointer is to select where the larger number is placed\nSolution public class MergeSortedArray { public static void merge(int[]nums1, int m, int[]nums2, int n) { //有序数组条件下，设置3个指针，分别放在nums1，nums2和集合数组的末尾 int i=m-1; int j=n-1; int point= nums1.length-1; //判断nums2是否遍历完成 while(j\u0026gt;=0){ //如果nums2中元素小于nums1中的，则把nums1里的元素放到point指向的位置 //之后移动i和point指针 if(i\u0026gt;=0 \u0026amp;\u0026amp; nums1[i]\u0026gt;nums2[j]){ nums1[point]=nums1[i]; i--; point--; } //nums2的元素较大情况直接把nums2中最大元素放置到数组最右端。 //移动j和point指针 else{ nums1[point]=nums2[j]; point--; j--; } } } ","permalink":"https://jackywang96.github.io/BlogSite/post/merge-sorted-array/","summary":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer.","title":"Merge Sorted Array"},{"content":"SQL experience\nWhen combining tables, the efficiency of simply using the where statement is lower than join, which is equivalent to full join left join (left join): Returns all records including all records in the left table and records with equal join fields in the right table.\nright join (right join): Returns all records including all records in the right table and records with equal join fields in the left table.\nInner join (equivalent connection or inner connection): only returns the rows with the same connection fields in the two tables.\nfull join (full outer connection): Return all records in the left and right tables and records with the same connection fields in the left and right tables.\nThe way to use Null is Where xxxx is null\nUse limit 1, 1 to filter the second and third data that meet the conditions. eg. the second largest, the second highest\n","permalink":"https://jackywang96.github.io/BlogSite/post/sql/","summary":"SQL experience\nWhen combining tables, the efficiency of simply using the where statement is lower than join, which is equivalent to full join left join (left join): Returns all records including all records in the left table and records with equal join fields in the right table.\nright join (right join): Returns all records including all records in the right table and records with equal join fields in the left table.","title":"SQL"},{"content":"Hi, I\u0026rsquo;m Jack 👋 This is my first post. Welcome to my Blog\n","permalink":"https://jackywang96.github.io/BlogSite/post/my-first-post/","summary":"Hi, I\u0026rsquo;m Jack 👋 This is my first post. Welcome to my Blog","title":"My First Post"},{"content":"Hi, I\u0026rsquo;m Jack 👋 👤 Currently Software Development Engineer at Department of Education Victoria.\n📷 Enthusiastic in fitness, photography, travelling, reading \u0026amp; video games.\n📄 Checkout my resume: English / [简体中文]\n✉️ Contact me at JackyWangMel96@gmail.com\n","permalink":"https://jackywang96.github.io/BlogSite/about/","summary":"Hi, I\u0026rsquo;m Jack 👋 👤 Currently Software Development Engineer at Department of Education Victoria.\n📷 Enthusiastic in fitness, photography, travelling, reading \u0026amp; video games.\n📄 Checkout my resume: English / [简体中文]\n✉️ Contact me at JackyWangMel96@gmail.com","title":"About"}]