[{"content":"Introduction 今天开始我将开始Java基础课程的教授。我会记录我每次备课的note并作为帖子发送至我的blog。\nContent 1. Java是什么样的语言 简单而言，使用JDK 的开发工具完成的java 程序，交给JRE 去运行\n- 面向对象: OOD（Object Oriented Programming）POP（Prodedure Oriented Programming）\n面向对象好在哪：POP小公司人少，所有安排事情是-你去干啥，他去干啥\nOOD: 大公司人多，分成多个部门，按职能分派任务\n三大特性：封装、继承、多态: 封装 用一个method隐藏赋值的过程、把属性变为private。只有外界用method去调用 public, private, default\n继承 inheritance； Child and parent class\n多态 overload\n- 健壮性: 吸收了C/C++ 语言的优点，但去掉了其影响程序健壮性的部分（如指针、 内存的申请与释放等），提供了一个相对安全的内存管理和访问机制。\n- 跨平台性: 跨平台性：通过Java 语言编写的应用程序在不同的系统平台上都可以运行。“Write once , Run Anywhere” 原理：只要在需要运行java 应用程序的操作系统上，先安装一个Java 虚拟机 (JVM Java Virtual Machine) 即可。由JVM 来负责Java 程序在该系统中的运行。\nJava 两种核心机制-Java VirtalMachine，Garbage Collection\n2. 基本语法 2.1 Java 中的名称命名规范 Package name: ：多单词组成时所有字母都小写：xxxyyyzzz Class、interface：所有单词的首字母大写：XxxYyyZzz 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz 常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ\n2.2 基本数据类型 变量按照数据类型来分： 基本数据类型：\n整型：byte \\ short \\ int \\ long\n浮点型：float \\ double\n字符型：char\n布尔型：boolean\n引用数据类型：\n类：class\n接口：interface\n数组：array\n2.3 字符串类型：String String 类型变量的使用：\nString 属于引用数据类型 。 声明String 类型变量时，使用一对\u0026quot;\u0026quot; 。 String 可以和8 种基本数据类型变量做运算， 且运算只能是连接运算；+ 运算的结果任然是String 类型。 2.4 强制类型转换 自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符：()，但可能造成精度降低或溢出, 格外要注意。 通常，字符串不能直接转换为基本类型，但通过基本类型对应的包装类则可以实现把字符串转换成基本类型。\n如：String a = “43”;\ninti= Integer.parseInt(a);\nboolean 类型不可以转换为其它的数据类型。\n2.5 运算符 % 取余 7%5=2\n++ a=2; b=a++; a=2;b=3\n扩展赋值运算符：+=, -=, *=, /=, %=\n逻辑运算符\na||b a\u0026amp;\u0026amp;b\n2.6 顺序结构 public class Test{ int num1 = 12; int num2 = num1 + 2; } // 错误形式： public class Test{ int num2 = num1 + 2; int num1 = 12; } 2.7 判断语句 2.7.1、分支语句：if-else 结构 if(X\u0026gt;=0){ return result } else if(x\u0026lt;0\u0026gt;){ return 1; } else{ return 0; } 2.8. 循环结构 2.8.1 for 循环 // 遍历100 以内的偶数, 获取所有偶数的和, 输出偶数的个数 int sum=0; int count=0; for(int i=1; i\u0026lt;=100; i++){ if(i%2==0){ System.out.println(i); sum += i; count++ } } System.out.println(\u0026ldquo;100 以内的偶数的和：\u0026rdquo; + sum); System.out.println(\u0026quot; 个数为：\u0026quot; + count);\n2.8.2 while 循环 class WhileTest{ public static void main(String[] args){ // 遍历100 以内的所有偶数 int i = 1; while(i \u0026lt;= 100){ if(i % 2 == 0){ System.out.println(i); } i++; } } } 2.8.3 do-while 循环 int i = 0; do { System.out.println(i); i++; } while (i \u0026lt; 5); 2.9. break、continue 的使用 break 语句用于终止某个语句块的执行\n{ ...... break; ...... } continue 的使用 continue 语句：continue 只能使用在循环结构中。\ncontinue 语句用于跳过其所在循环语句块的一次执行，继续下一次 循环。\nreturn return：并非专门用于结束循环的，它的功能是结束一个方法。当一 个方法执行到一个return 语句时，这个方法将被结束。\n与break 和continue 不同的是，return 直接结束整个方法，不管这个 return 处于多少层循环之内。\n课后作业 目标\n需求说明\n基本金和收支明细的记录\n模拟实现一个基于文本界面的《家庭记账软件》。\n主要掌握以下知识点： 变量的定义， 基本数据类型的使用， 循环语句， 分支语句， 方法声明、调用和返回值的接收， 简单的屏幕输出格式控制 模拟实现基于文本界面的《家庭记账软件》。 该软件能够记录家庭收入、支出，并能够打印收支明细表。\n项目采用分级菜单方式。主菜单如下：\n\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- 家庭收支记账软件\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-\n收支明细 登记收入 登记支出 退 出 请选择(1-4): ","permalink":"https://jackywang96.github.io/BlogSite/post/java/javal-lesson1/","summary":"Introduction 今天开始我将开始Java基础课程的教授。我会记录我每次备课的note并作为帖子发送至我的blog。\nContent 1. Java是什么样的语言 简单而言，使用JDK 的开发工具完成的java 程序，交给JRE 去运行\n- 面向对象: OOD（Object Oriented Programming）POP（Prodedure Oriented Programming）\n面向对象好在哪：POP小公司人少，所有安排事情是-你去干啥，他去干啥\nOOD: 大公司人多，分成多个部门，按职能分派任务\n三大特性：封装、继承、多态: 封装 用一个method隐藏赋值的过程、把属性变为private。只有外界用method去调用 public, private, default\n继承 inheritance； Child and parent class\n多态 overload\n- 健壮性: 吸收了C/C++ 语言的优点，但去掉了其影响程序健壮性的部分（如指针、 内存的申请与释放等），提供了一个相对安全的内存管理和访问机制。\n- 跨平台性: 跨平台性：通过Java 语言编写的应用程序在不同的系统平台上都可以运行。“Write once , Run Anywhere” 原理：只要在需要运行java 应用程序的操作系统上，先安装一个Java 虚拟机 (JVM Java Virtual Machine) 即可。由JVM 来负责Java 程序在该系统中的运行。\nJava 两种核心机制-Java VirtalMachine，Garbage Collection\n2. 基本语法 2.1 Java 中的名称命名规范 Package name: ：多单词组成时所有字母都小写：xxxyyyzzz Class、interface：所有单词的首字母大写：XxxYyyZzz 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz 常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ","title":"Java Lesson1 Note"},{"content":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Spark core module Spark Core: The most basic and core functions of Spark are provided in Spark Core\nSpark SQL: Spark SQL is the component that Spark uses to manipulate structured data. With Spark SQL, users can query data using SQL or the Apache Hive version of the SQL dialect (HQL).\nSpark Streaming: Spark Streaming is a component on the Spark platform that performs stream computing for real-time data, and provides a rich API for processing data streams.\n2. Spark running model Local model:\nAn environment that executes Spark code locally without requiring any other node resources Standalone model:\nSubmit the application to the corresponding cluster for execution, using the cluster mode running on Spark\u0026rsquo;s own nodes, which is what we call the standalone deployment (Standalone) mode.\nOnline monitor: Use the Master resource to monitor the Web UI interface: http://linux1:8080\nYarn model:\nThe Yarn mode can utilize the resources of the Hadoop cluster, and the Spark application is submitted to the YARN cluster. Provides functions such as dynamic resource allocation and job recovery, suitable for large-scale production environments.\nThe Standalone mode is more suitable for small-scale testing and development environments.\nK8S \u0026amp; Mesos model:\nMesos is an open source distributed resource management framework under Apache. It is known as the kernel of a distributed system. It is widely used in Twitter and manages the application deployment on more than 300,000 Twitter servers.\n","permalink":"https://jackywang96.github.io/BlogSite/post/big-data/spark-learning-2/","summary":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Spark core module Spark Core: The most basic and core functions of Spark are provided in Spark Core\nSpark SQL: Spark SQL is the component that Spark uses to manipulate structured data. With Spark SQL, users can query data using SQL or the Apache Hive version of the SQL dialect (HQL).\nSpark Streaming: Spark Streaming is a component on the Spark platform that performs stream computing for real-time data, and provides a rich API for processing data streams.","title":"Big Data Learning Day-2"},{"content":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Data processing tool Spark and Handoop Apache Spark: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It offers support for various data processing tasks, including SQL queries, machine learning, and graph processing, through built-in libraries like Spark SQL, MLlib, and GraphX.\nApache Hadoop: Apache Hadoop is an open-source, distributed computing framework for processing and storing large amounts of data on clusters of commodity hardware. The core components of Hadoop are the Hadoop Distributed File System (HDFS) and the MapReduce programming model. HDFS is a distributed file system that provides high-throughput access to application data, while MapReduce is a parallel data processing model that simplifies large-scale data processing across distributed clusters.\n2. Data storage tool As big data involves large volumes of data, it requires specialized data storage systems such as HDFS, Amazon S3, or Google Cloud Storage.\n3. Data visualization and analysis Once the data has been processed, it needs to be analyzed and presented in a meaningful way. There are several tools available for data visualization and analysis, such as Tableau, Power BI, and Apache Superset.\n4. Data governance and security As big data involves sensitive data, it\u0026rsquo;s important to have strong data governance and security measures in place. This includes access control, data masking, and encryption.\n","permalink":"https://jackywang96.github.io/BlogSite/post/big-data/spark-learning-1/","summary":"Introduction After this article, I will record some experience of learning big data.\nContent 1. Data processing tool Spark and Handoop Apache Spark: Apache Spark is an open-source, distributed computing system designed for big data processing and analytics. It offers support for various data processing tasks, including SQL queries, machine learning, and graph processing, through built-in libraries like Spark SQL, MLlib, and GraphX.\nApache Hadoop: Apache Hadoop is an open-source, distributed computing framework for processing and storing large amounts of data on clusters of commodity hardware.","title":"Big Data Learning Day-1"},{"content":"Introduction Tailwind CSS is a CSS styling library that provides us with all the building blocks we need to build custom designs without using custom styles.\nFeature Tailwind will automatically delete all unused CSS when building production files, so its size in the actual production environment is very small, and it is considered to be the CSS style library with the most design sense.\nInstallation //Install Tailwind CSS npm install -D tailwindcss npx tailwindcss init //Add the paths to all of your template files in your tailwind.config.js file. module.exports = { content: [\u0026#34;./src/**/*.{html,js}\u0026#34;], theme: { extend: {}, }, plugins: [], } //Add the Tailwind directives to your CSS @tailwind base; @tailwind components; @tailwind utilities; //Start the Tailwind CLI build process npx tailwindcss -i ./src/input.css -o ./dist/output.css --watch ","permalink":"https://jackywang96.github.io/BlogSite/post/tailwindcss/","summary":"Introduction Tailwind CSS is a CSS styling library that provides us with all the building blocks we need to build custom designs without using custom styles.\nFeature Tailwind will automatically delete all unused CSS when building production files, so its size in the actual production environment is very small, and it is considered to be the CSS style library with the most design sense.\nInstallation //Install Tailwind CSS npm install -D tailwindcss npx tailwindcss init //Add the paths to all of your template files in your tailwind.","title":"TailwindCSS"},{"content":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer. In this case, a third pointer is required at length of nums1. The role of the third pointer is to select where the larger number is placed\nSolution public class MergeSortedArray { public static void merge(int[]nums1, int m, int[]nums2, int n) { //有序数组条件下，设置3个指针，分别放在nums1，nums2和集合数组的末尾 int i=m-1; int j=n-1; int point= nums1.length-1; //判断nums2是否遍历完成 while(j\u0026gt;=0){ //如果nums2中元素小于nums1中的，则把nums1里的元素放到point指向的位置 //之后移动i和point指针 if(i\u0026gt;=0 \u0026amp;\u0026amp; nums1[i]\u0026gt;nums2[j]){ nums1[point]=nums1[i]; i--; point--; } //nums2的元素较大情况直接把nums2中最大元素放置到数组最右端。 //移动j和point指针 else{ nums1[point]=nums2[j]; point--; j--; } } } ","permalink":"https://jackywang96.github.io/BlogSite/post/fast-slow-pointer-floyd-circle-method/fast-slow-pointer/","summary":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer.","title":"Fast,Slow pointer--Floyd circle method"},{"content":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer. In this case, a third pointer is required at length of nums1. The role of the third pointer is to select where the larger number is placed\nSolution public class MergeSortedArray { public static void merge(int[]nums1, int m, int[]nums2, int n) { //有序数组条件下，设置3个指针，分别放在nums1，nums2和集合数组的末尾 int i=m-1; int j=n-1; int point= nums1.length-1; //判断nums2是否遍历完成 while(j\u0026gt;=0){ //如果nums2中元素小于nums1中的，则把nums1里的元素放到point指向的位置 //之后移动i和point指针 if(i\u0026gt;=0 \u0026amp;\u0026amp; nums1[i]\u0026gt;nums2[j]){ nums1[point]=nums1[i]; i--; point--; } //nums2的元素较大情况直接把nums2中最大元素放置到数组最右端。 //移动j和point指针 else{ nums1[point]=nums2[j]; point--; j--; } } } ","permalink":"https://jackywang96.github.io/BlogSite/post/merge-sorted-array/","summary":"Introduction This article is about how to use double pointers to solve problems such as merge array\nProblem details Leetcode 88. Merge Sorted Array Analysis and thought Because it is a sorted array, add pointers at the end of nums1 and nums2 respectively. The values at the positions of the two pointers are compared each time. Copy the larger number to the last bit of nums1. After that, move the pointer.","title":"Merge Sorted Array"},{"content":"SQL experience\nWhen combining tables, the efficiency of simply using the where statement is lower than join, which is equivalent to full join left join (left join): Returns all records including all records in the left table and records with equal join fields in the right table.\nright join (right join): Returns all records including all records in the right table and records with equal join fields in the left table.\nInner join (equivalent connection or inner connection): only returns the rows with the same connection fields in the two tables.\nfull join (full outer connection): Return all records in the left and right tables and records with the same connection fields in the left and right tables.\nThe way to use Null is Where xxxx is null\nUse limit 1, 1 to filter the second and third data that meet the conditions. eg. the second largest, the second highest\n","permalink":"https://jackywang96.github.io/BlogSite/post/sql/","summary":"SQL experience\nWhen combining tables, the efficiency of simply using the where statement is lower than join, which is equivalent to full join left join (left join): Returns all records including all records in the left table and records with equal join fields in the right table.\nright join (right join): Returns all records including all records in the right table and records with equal join fields in the left table.","title":"SQL"},{"content":"Hi, I\u0026rsquo;m Jack 👋 This is my first post. Welcome to my Blog\n","permalink":"https://jackywang96.github.io/BlogSite/post/my-first-post/","summary":"Hi, I\u0026rsquo;m Jack 👋 This is my first post. Welcome to my Blog","title":"My First Post"},{"content":"Hi, I\u0026rsquo;m Jack 👋 👤 Currently Software Development Engineer at Department of Education Victoria.\n📷 Enthusiastic in fitness, photography, travelling, reading \u0026amp; video games.\n📄 Checkout my resume: English / [简体中文]\n✉️ Contact me at JackyWangMel96@gmail.com\n","permalink":"https://jackywang96.github.io/BlogSite/about/","summary":"Hi, I\u0026rsquo;m Jack 👋 👤 Currently Software Development Engineer at Department of Education Victoria.\n📷 Enthusiastic in fitness, photography, travelling, reading \u0026amp; video games.\n📄 Checkout my resume: English / [简体中文]\n✉️ Contact me at JackyWangMel96@gmail.com","title":"About"}]