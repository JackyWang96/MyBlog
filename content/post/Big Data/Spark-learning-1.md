---
title: "Big Data Learning Day-1"
date: 2023-03-27T02:52:22+11:00
draft: false
---

# Introduction
After this article, I will record some experience of learning big data.



# Content
## 1. Data processing tool

### Spark and Handoop

- **Apache Spark:**

Apache Spark is an open-source, distributed computing system designed for big data processing and analytics.  It offers support for various data processing tasks, including SQL queries, machine learning, and graph processing, through built-in libraries like Spark SQL, MLlib, and GraphX.

- **Apache Hadoop:**

Apache Hadoop is an open-source, distributed computing framework for processing and storing large amounts of data on clusters of commodity hardware. The core components of Hadoop are the Hadoop Distributed File System (HDFS) and the MapReduce programming model. HDFS is a distributed file system that provides high-throughput access to application data, while MapReduce is a parallel data processing model that simplifies large-scale data processing across distributed clusters.

## 2. Data storage tool

As big data involves large volumes of data, it requires specialized data storage systems such as HDFS, Amazon S3, or Google Cloud Storage.

## 3. Data visualization and analysis

Once the data has been processed, it needs to be analyzed and presented in a meaningful way. There are several tools available for data visualization and analysis, such as Tableau, Power BI, and Apache Superset.

## 4. Data governance and security

As big data involves sensitive data, it's important to have strong data governance and security measures in place. This includes access control, data masking, and encryption.


